# -*- coding: utf-8 -*-
"""[Week2] Pytorch_Basics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ook97A4DA4caRSKL5jv_H-dfMVaBbUUe

# Basics of Pytorch

Pytorch is one of the most used Python libraries for deep learning.  
Pytorch will help you to accelerate the research by making them computationally faster and less expensive.  
This material will guide you on how PyTorch works, and how you can get started with it.

## Tables of Content
1.  Getting Started with Pytorch
    + Installation of Pytorch
    + What is Pytorch?
2. Basics of Pytorch
    + Instruction to Tensors
    + Operations on Tensors
3. Autograd : Automatic Differentiaton
    + Record grad_fn on Tensors
    + Calcuate Gradient through Backward
4. Reference

### Getting Started with Pytorch
#### Installation of Pytorch
Visit https://pytorch.org/get-started/locally/ to select your preferences and run the install command  
To install the PyTorch binaries, you will need to use at least one of two supported package managers: **Anaconda** and **pip**  
PyTorch on Windows only supports **Python 3.x**(Python 2.x is not supported)

#### What is Pytorch?
Pytorch is a **Python-based scientific computing package** that features several advantages:
+ A replacement for NumPy to use the power of GPUs
+ a deep learning research platform that provides maximum flexibility and speed  
+ Provides distributed training which makes it possible to use multiple GPUs. <br/> Distributed training enables to process larger batches of input data, reducing the computation time.
+ It creates dynamic computation graphs meaning that the graph will be created on the fly
+ Simple Interface: It offers easy to use API, thus it is very simple to operate and run like Python.

## Basics of Pytorch
### Instroduction to Tensors
Tensors are multidimensional arrays.   
Tensors are similar to NumPy’s ndarrays, but it can also be used on a GPU to accelerate computing.(this is not the case with NumPy arrays)  
See how we can create a Tensor in Pytorch and compares it Ndarray from Numpy:
"""

import numpy as np
import torch

a = np.array(1) # initializing a numpy array
b = torch.tensor(1) # initializing a tensor

print(a, type(a))
print(b, type(b))

"""Numpy ndarray is easily convertible to Pytorch tensor, and vice versa"""

a = np.array(1)
a_tensor = torch.from_numpy(a)
b = torch.tensor(1)
b_ndarray = b.numpy()
print(type(a_tensor), type(b_ndarray))

"""Construct a 5x3 matrix, uninitialized:"""

x = torch.empty(5, 3)
print(x)

"""Construct a 5x3 matrix, uninitialized:"""

x = torch.rand(5, 3)
print(x)

"""Construct a matrix filled zeros and of dtype long:"""

x = torch.zeros(5, 3, dtype=torch.long)
print(x)

"""Construct a tensor directly from data:"""

x = torch.tensor([5.5, 3])
print(x)

"""Create a tensor based on an existing tensor.  
These methods will reuse properties of the input tensor, e.g. dtype, unless new values are provided by user
"""

x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes
print(x)

x = torch.randn_like(x, dtype=torch.float)    # override dtype!
print(x)                                      # result has the same size

"""Get its size:"""

print(x.size())

"""### Operations on Tensors

There are multiple syntaxes for operations. In the following example, we will take a look at the addition operation.  
**Addition: syntax 1**
"""

y = torch.rand(5, 3)
print(x + y)

"""**Addition: syntax 2**"""

print(torch.add(x, y))

"""See how basic operations are performed on tensor in Pytorch"""

a = torch.randn(3,3)
b = torch.randn(3,3)
# matrix addition
print(torch.add(a,b), '\n')

# matrix subtraction
print(torch.sub(a,b), '\n')

# matrix multiplication
print(torch.mm(a,b), '\n')

# matrix division
print(torch.div(a,b))

""" **Calculating transpose** is also similar to NumPy:"""

# original matrix
print(a, '\n')

# matrix transpose
torch.t(a)

"""**Concatenating Tensors**  
Let’s say we have two tensors as shown below:
"""

a = torch.tensor([[1,2],[3,4]])
b = torch.tensor([[5,6],[7,8]])
print(a, '\n')
print(b)
c0 = torch.cat((a,b),dim=0) # concate tensors at dimension 0
print(c0)
print(c0.shape)
c1 = torch.cat((a,b),dim=1) # concate tensors at dimension 1
print(c1)
print(c1.shape)

"""**Reshaping Tensors**  
Let’s say we have the following tensor:
"""

a = torch.randn(2,4)
print(a)
a.shape

"""You can use either of two functions : **torch.view()** and **torch.reshape()**  
**torch.view()** returns tensor which share the underling data with the original tensor  
On the other hand, **torch.reshape()** may return a copy or a view of the original tensor.
"""

b = a.reshape(1,8)
c = a.view(1,8)
print(b)
print(b.shape)
print(c)
print(c.shape)

"""**Indexing**  
You can use standard NumPy-like indexing
"""

a = torch.randn(2,4)
print(a[:,:2].shape)

"""If you have a one element tensor, use .item() to get the scalar value."""

x = torch.randn(1)
print(x)
print(x.item())

"""**CUDA Tensors**  
Tensors can be moved onto any device using the .to method.
"""

# let us run this cell only if CUDA is available
# We will use ``torch.device`` objects to move tensors in and out of GPU
if torch.cuda.is_available():
    device = torch.device("cuda:0")          # a CUDA device object (to GPU No.0)
    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU
    x = x.to(device)                       # or just use strings ``.to("cuda")``
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))       # ``.to`` can also change dtype together!

"""## Autograd : Automatic Differentiaion

Remember in your calculas class when you learn about gradient and differentation?  
In machine learning, it is necessary to compute the gradients of loss function in neural networks.  
However, it is inefficient to compute gradients of large composite function in very high dimensional spaces.  
Fortunately, PyTorch provides a technique called automatic differentiation.   
Pytorch records all the operations enabling it to compute gradients backward.  
This technique will help you to save time on each epoch

Let’s look at an example to understand how the autograd works!

### Tensors with Gradients
Once you create tensors specifing the option **requires_grad** as **True**,  
gradients are stored for this particular tensor whenever you perform some operations  
When you finish your computation you can call .backward() and have all the gradients computed automatically.  
The gradient for this tensor will be accumulated into .grad attribute.
"""

# Create a tensor and set requires_grad=True to track computation with it
x = torch.ones(2, 2, requires_grad=True)
print(x)

"""Let’s now perform some operations on the defined tensor  
Each tensor has a .grad_fn attribute that references a Function that has created the Tensor  
(except for Tensors created by the user - their grad_fn is None).


"""

y = x + 2
print(y)
print(y.grad_fn)

"""Do more operations on y"""

z = y * y * 3
out = z.mean()

print(z, out)

""".requires_grad_( ... ) changes an existing Tensor’s requires_grad flag in-place.  
The input flag defaults to False if not given
"""

a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)

"""### Calcuate Gradient through Backward

If you want to compute the derivatives, you can call **.backward()** on a Tensor.  
Once you call **.backward()**, its gradient is sotred in the attribute **.grad**  
If Tensor is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to backward()  
However, if it has more elements, you need to specify a gradient argument that is a tensor of matching shape.
"""

a = torch.ones((2,2), requires_grad=True) # initializing a tensor
b = a + 5 # performing operations on the tensor
c = b.mean()

# back propagating
c.backward()
# check computed gradients
print(a.grad)

"""You can also stop autograd from tracking history on Tensors with **.requires_grad=False**
either by wrapping the code block in with torch.no_grad():
"""

print(a.requires_grad)
print((a ** 2).requires_grad)

with torch.no_grad():
    print((a ** 2).requires_grad)

"""To prevent tracking history, (maybe for memory issue)  
you can use **.detach()** to get a new Tensor with the same content but that does not require gradients.  
Note that **.clone()** functon also returns a copy but that does trackes the gradients.
"""

print(a.requires_grad)
b = a.detach()
c = a.clone()
print(b.requires_grad)
print(c.requires_grad)
print((a==b).all())
print((a==c).all())

"""### Reference

[What is PyTorch?] https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py  
[AUTOGRAD: AUTOMATIC DIFFERENTIATION] https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py  
[A Beginner-Friendly Guide to PyTorch and How it Works from Scratch] https://www.analyticsvidhya.com/blog/2019/09/introduction-to-pytorch-from-scratch/  
[What is PyTorch and how does it work?] https://hub.packtpub.com/what-is-pytorch-and-how-does-it-work/
"""